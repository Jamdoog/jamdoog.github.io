[ { "title": "Configuring FreeIPA LDAP with Ansible Automation Platform 2.5", "url": "/posts/configuring-freeipa-ldap-with-ansible-automation-platform-25/", "categories": "", "tags": "Linux,, Ansible,, RedHat,, FreeIPA", "date": "2024-12-31 00:00:00 +0000", "snippet": "IntroductionThis is a quick blog post to highlight integrating FreeIPA‚Äôs LDAP(S) into the recent Ansible Automation Platform 2.5.This also applies to RedHat‚Äôs Identity Management (IDM) which is their commercial version of FreeIPA.While documentation of the LDAP provider exists, I could only find 1 example of this online from a few years ago.This post is more of a reference point then a tutorial.Configuring Create a bind user and give it appropriate permissions to search the database (Optional) Create a user group that is required for someone to log into the platform with (Optional) Create a user group that will grant administrative privilegesOnce on the platform, you can use this configuration as a point of reference: LDAPS may not work out the box if you do not trust the root certificate of the IPA serverEnroll your servers in your IPA realm. It will automatically fetch and trust the CA!Group RulesRules in 2.5 has changed since 2.4, with it being a little less intuitive in my opinion. Here are the rules I deployed:What this reads as: Name of the rule Trigger (In this case: A directory group) Operation (AND/OR) This is ignored if there is only 1 provided Groups (CN path to appropriate group) Revoke (Do the opposite. In this case; don‚Äôt allow login)Here is an example of a CN I used for the allow login group. cn=aap_users,cn=groups,cn=accounts,dc=jamdoog,dc=gra" }, { "title": "QEMU Guest Agent on pfSense", "url": "/posts/guest-agent-on-pfsense/", "categories": "", "tags": "Networking, Linux, UNIX, FreeBSD, Proxmox, Virtual Machine", "date": "2022-05-26 00:00:00 +0100", "snippet": "A piece of software I always found fascinating about hypervisors was the concept of a guest agent. The first example I recall was file sharing from a VMWare Host to Guest on Workstation which seemed genius, removing the need for either USB or Network approaches to traditional file sharing.Forward to now, my virtualization needs have since expanded to a completely seperate home server with Proxmox as a chosen Hypervisor. Proxmox is a Free and Open Source software suite for managing KVM/QEMU virtualization in addition to legacy LXC containers.This is great and the qemu-guest-agent project is brilliant, however it has always posed 1 problem: A lacking UNIX port. It was to great suprise to recently learn that their has been a port development over the past few years that has reached maturity in the latest FreeBSD version. This finally meant all of my guest‚Äôs had proper shutdown capability, no more hanging on the occasional host reboot. ü•≥InstallInstallation of the agent, while not point and click, is relatively easy.I should link this thread for reference to the instructions.Someone also made a auto-installer script.To begin with, open up your console. We need to access the shell so type in 8.Then to install the QEMU Guest Agent package, it is 1 simple command:pkg install qemu-guest-agent.After this, rc.local needs to be modified:echo 'qemu_guest_agent_enable=\"YES\"' &gt;&gt; /etc/rc.local.confecho 'qemu_guest_agent_flags=\"-d -v -l /var/log/qemu-ga.log\"' &gt;&gt; /etc/rc.local.confFinally, we need to start the service on boot:echo \"#!/bin/sh\" &gt;&gt; /usr/local/etc/rc.d/qemu-agent.shecho \"sleep 3\" &gt;&gt; /usr/local/etc/rc.d/qemu-agent.shecho \"service qemu-guest-agent-start\" &gt;&gt; /usr/local/etc/rc.d/qemu-agent.shFinally, ensure the agent is enabled on Proxmox itself.After this, you will have to hard stop and start the VM. If you are like me and rely on it for a network connection, open up a root shell on the host and type:qm stop &lt;vmid&gt;; qm start &lt;vmid&gt;. This will stop and start it, regaining network connectivity when it‚Äôs up again.Result!The open source community has blessed us one again.The easiest way of testing if the agent is functional is by viewing the summary of the VM. It should display all of the IP‚Äôs of the VM." }, { "title": "Taking advantage of free S3 compatible storage on a ¬£0.30 VPS", "url": "/posts/scaleway-s3-stardust/", "categories": "", "tags": "Container, Docker, Linux, Networking, Web", "date": "2021-09-30 00:00:00 +0100", "snippet": "This post is not meant to be used in a production workflow. I just wanted to show how you can host some data for practically free while being under your OWN termsI have a problem with renting too many virtual servers purely because they are cheap and after recently discovering Scaleway‚Äôs offer of a 1GB VPS for ¬£0.30 a month, I had to find a use for some of the idling compute.These ‚ÄúStardust‚Äù instances are little KVM boxes with a 100M port and the optional ability to include IPv4 at an extra 1 euro or so. Having previously used (and still do) their dedicated server offerings, it was incredible deal that I haven‚Äôt seen mentioned much. IPv6 adoption may still be a dream ~ but for now, it‚Äôs good enough.Cheap enough as it is, but they don‚Äôt tell you about removing the ‚ÄúFlexible IP‚Äù option. Pay hourly for a floating IP? Awesome.Seriously, that‚Äôs a monthly price. For 1GB of ram!The thing which really caught my eye though was the free object storage. A whole 75GB! Not only can you host HTTP sites from them, but you can interact with them completely free via this instance. If it was a 1 gigabit port I wouldn‚Äôt want to imagine the abuse this would cause.‚ÄîPenalties of going IPv6 onlyOf course without proper adoption, IPv6 presents natural challenges. For instance, I couldn‚Äôt download docker-compose because it‚Äôs stored on Github. Seriously?Translation mechanisms such as DNS64 exist but they‚Äôre not good enough. In this case, the best bet is to rent a floating IP for an hour if needed while configuring and later remove it once done. Save the headache, please‚Ä¶One of the things I found really interesting about Scaleway‚Äôs approach to floating IP‚Äôs was that they NAT your traffic. That‚Äôs right, not hacky scripts to mess with. Your traffic just ‚ú® works ‚ú® (NAT‚Äôs a good thing???)Anyway, the main thing I was looking for was that Scaleway would support IPv6 on their S3-compatible object storage which they did! But now the next problem was Docker. Incase you aren‚Äôt aware (blissful ignorance), docker was not built up on the foundation of IPv6 and thus has lacking support. Infact, people conciously chose to NAT IPv6, me included. I‚Äôd say I‚Äôm sorry but well, I‚Äôm not.Wido den Hollander wrote a blog post about a makeshift IPv6 NAT solution with ip6tables. Quite frankly, this was the fastest and easiest method of achieving this. The headache of using a real address was not something I was prepared to do.Installing DockerInstalling docker is made pain free by a quick bash script they provide. This is perhaps the lazy way but I‚Äôve yet to find it has failed me.curl https://get.docker.com | bashThat‚Äôs it.Configuring DockerThe method to NAT IPv6 I chose meant that it wouldn‚Äôt support docker-compose without some work. Quite honestly after debugging this for maybe 4 hours it was already more time then I was wanting to spend on this science experiment. As such, I created manual docker files. But before this I had to configure docker.As a quick note, this ‚Äútutorial‚Äù is taking place on Debian 11. I can‚Äôt speak for Podman on RHEL based distro‚Äôs but I would imagine it‚Äôs a similar ordeal?There‚Äôs two things we need to do, modify the docker startup parameters and create a ip6tables rules to NAT the traffic.To begin with, I modified /usr/lib/systemd/system/docker.service and appended the following flags to ExecStart:--ipv6 --fixed-cidr-v6=\"fd00::/64\"The whole command then looked like:ExecStart=/usr/bin/dockerd --ipv6 --fixed-cidr-v6=\"fd00::/64\" -H fd:// --containerd=/run/containerd/containerd.sockAfter this, we need to make systemd see the change:systemctl daemon-reloadThen finally we can restart the docker service:systemctl restart dockerNext, we need to just create the NAT rules on ip6tables:ip6tables -t nat -A POSTROUTING -s fd00::/64 -j MASQUERADE Again, this should ONLY be used for testing purposes. For production IPv6 Prefix Delegation is the route to go down.Wido himself mentions that this is not for production. I recommend you read his mentioned post on prefix delegation if you was to use this for cirtical applications.DockerfilesI found there was a lot of issues while trying to deploy Nextcloud with Docker. Genuinely, the official docker-compose files were not working for me out the box but eventually I got a working configuration.RedisThe most simple one:docker run -itd --name redis --restart=always redisMariaDBThe most recent version of MariaDB broke support for Nextcloud. It‚Äôs something to do with an InnoDB incompatability. I‚Äôll be honest, I‚Äôm not a database person. The fix was just to run an older version, 10.5 specificallydocker run -itd --name db -v nextcloud_db:/var/lib/mysql -e \"MYSQL_ROOT_PASSWORD=snip\" -e \"MYSQL_PASSWORD=snip\" -e \"MYSQL_DATABASE=nextcloud\" -e \"MYSQL_USER=nextcloud\" --restart=always mariadb:10.5NextcloudWhen looking about the best way to link the databases to Nextcloud, apparently the --link flag is deprecated right now with potential to be removed in the future. However, the official Nextcloud docker-compose images still use it so I based it off this:docker run -itd --name nextcloud -v nextcloud_nextcloud:/var/www/html -p 80:80 --link db --link redis --restart always nextcloudYou can bind the instance to a specific IP as opposed to 0.0.0.0 by modifying the port parameter with an IP. For example, I bound this to ONLY listen on my wireguard instance.docker run -itd --name nextcloud -v nextcloud_nextcloud:/var/www/html -p 192.168.183.2:80:80 --link db --link redis --restart always nextcloudAfter all of this has been ran, we will be presented with the nextcloud page. All you need to do is input the credentials you create above and you‚Äôre good to go!Adding the Object Storage to NextcloudBy default, Nextcloud does not support external storage which I find quite strange. But thankfully it is easy enough to add it, without any hacks as a bonus.We need to enable the ‚Äúapp‚Äù and then configure the bucket details in settings.Head to appClick ‚ÄúEnable‚Äù on the External Storage Support indexHead to settingsExternal StorageInput appropriate dataAt this point we need to get the information for our bucket and credentials to use. This will use API keys which you can generate on the Scaleway dashboard The information for the bucket will depend on the region, you can find it on your bucket settings. For example, my Paris instance uses the following:Bucket: &lt;bucketname&gt;Hostname: s3.fr-par.scw.cloudPort: 443Region: fr-parEnable SSL: yesFollowed by the API keys you generated on the dashboard. Assuming everything goes well, a green circle should appear to the left which of course means you have authenticated correctly.Using the object storageSimple, go to your files tab and you should see a folder with the name you called the external storage:This even works on mobile which is quite neat actually.Scaleway charges 1 eurocent per GB, which isn‚Äôt quite as good a rate as Backblaze but we have the added benefit of no egress fee‚Äôs (damn you AWS) due to being on their cloud platform. However, with 75GB of free storage it‚Äôs hard to look away from Scaleway for small-time data storage. This whole experiment could cost you literally 37 cent a month! seriously, why is this so cheap" }, { "title": "Securing SSH with a Jump Host and Wireguard", "url": "/posts/securing-ssh-with-a-jump-host/", "categories": "", "tags": "Linux, Networking", "date": "2021-08-17 00:00:00 +0100", "snippet": "*Disclaimer: At the time of writing this, Wireguard is not in stable repositories of some distribution. ***Secure Shell (SSH) **is a useful utility which allows administrators to remotely control a plethora of devices ranging from routers to UNIX servers without exposing credentials in plaintext. Today, the majority of LINUX and UNIX based systems run a implementation of SSH called OpenSSH. Developed by the talented people over at the OpenBSD project, their implementation of SSH is completely opensource and free to the public.On the contrary, Wireguard is a recently released communication protocol which is incredibly lightweight and simple. By working with purely key-based authentication, we can deploy hosts within seconds without any hassle of port forwarding on the client. As a result of this, people stuck behind a CG-NET deployment or even an IPv6 only network can tunnel without being concerned with opening ports.By combining the two technologies, we can create a Virtual Private Network (VPN) by which the SSH service is exposed on, but not the WAN. By doing this, it drastically reduces the attack surface of SSH. I suspect this way of authentication is more useful for high security zones such as a co-location facility, however, there‚Äôs no harm in securing your own infrastructure‚Ä¶ right?Outlining how a Jump host functionsSSH utilises the jumper node as a intermediary for trafficA jump host will act as a intermediary between yourself and the target machine. You may jump more then once, but in this example there will only be 1 added hop. Ordinarily the user would not be able to access the nodes, however, by utilizing the Jump node as a intermediary we can then SSH into a node on the VPN.Installing WireguardWith the release of Wireguard into the Linux kernel 5.6, I would like to consider it a stable piece of software. In fact, with the addition of it in stable distributions such as Debian 11 it may even be classed as so. However, common stable distributions may lack behind the new 5.10 LTS kernel which means an equivalent of back ports has to be used.Debian 11 ‚ÄòBullseye‚ÄôAs mentioned, Debian 11 runs the latest 5.10 LTS kernel which means it has native support for Wireguard. As such, installing it is simple:sudo apt install wireguardDebian 10 ‚ÄòBuster‚ÄôA little more complex, requires us to install the back ports repository and Linux headers. To do this, we must create a entry in /etc/apt/sources.list.d containing the repository.echo \"deb http://deb.debian.org/debian buster-backports main\" | sudo tee /etc/apt/sources.list.d/backports.listThen we need to update our information for package sources:sudo apt updateAnd finally followed by the install.sudo apt install wireguard linux-headers-$(uname -r)When I was installing this on some of my routers, I noticed that I could not find the correct linux-headers package. In my case, this was because I had previously updated my kernel without rebooting. Rocky Linux 8.4 ‚ÄòGreen Obsidian‚ÄôSimilar to Debian 10, Wireguard is not present in the base repository list nor Linux kernel until a new release comes out on the 5.10 LTS kernel. So for now, we must install both the epel-release and elrepo-release repositories.sudo dnf install epel-release elrepo-releaseI was happy to read that the epel-release repository was maintained by the Fedora team. However, the elrepo repository appears to be entirely third party.Next, we just have to install the packages:sudo dnf install kmod-wireguard wireguard-toolsUbuntu Server 20.04 LTS ‚ÄòFocal Fossa‚ÄôWhile I‚Äôm not an Ubuntu person, I am sometimes jealous of the up to date package repositories‚Ä¶ Similar to Debian 11, we just have to install the wireguard package.sudo apt install wireguard‚ÄîConfiguring WireguardWireguard uses a public key authentication system. This means that each side of the tunnel has a store of keys to encrypt traffic with that only the correct peer may decrypt.Each peer communicate in a secure manner by encrypting it‚Äôs traffic with the peer‚Äôs public keyComical drawing aside, the above illustration displays a abstracted view of the transaction. Do not that this transaction will happen on both sides, it is not a client-server model.On both the Jumper and the endpoint, we need to generate a private key. This will create a public key based off it which we will need to take note of. This can be accomplished with the command:wg genkeyYou‚Äôll need to do this command twice, and take note of the keys generated.Once we have the keys, we can go ahead and configure our Wireguard configuration files. By default, Wireguard will read from the /etc/wireguard directory. In here, we want to create a file called wg0.conf and configure our tunnel.sudo nano /etc/wireguard/wg0.confJumperSome things to note about the configuration of Wireguard is that it fully supports dual stack networking. As such, not only can your VPN have IPv6 but it can also tunnel over IPv6! Just ensure you encapsulate the endpoint variable in [] if you plan to use IPv6. In anycase, here is my configuration file:[Interface]PrivateKey = 4IUXksWM2WyPWr3jAmmbRDxLNdDTDKTiOfIr1pLhaEg=Address = 10.40.0.1/24ListenPort = 12342Example configuration fileThen we can bring up the interface and view the public key it created. We will need to note this down as it‚Äôs required for our endpoint to encrypt traffic with.wg-quick up wg0We can see that we received a basic verbose output stating what commands have been executed:It will execute commands for us to create the interfaceNow we can retreive the public key of this configuration file. Actually, the command will show us more information then this but as of right now it‚Äôs only useful for this,sudo wg showThe command will output the public key and other helpful informationOnce we have this public key, we are ready to configure the endpoint.EndpointSimilar to before, we need to edit /etc/wireguard/wg0.conf but this time we have the addition of a peer notation thanks to the public key we generated above.[Interface]PrivateKey = 0HGqP0XNeXz3AWOniHwkNeBXQIsMiajiHl5+5uyBilA=Address = 10.40.0.2/24ListenPort = 12342[Peer]PublicKey = wiPoP3ybHBP+h7kfvO3r9vbRT9DON3W5FWbDlLAfb3E=AllowedIPs = 10.40.0.1/32Endpoint = 10.10.69.54:12342PersistentKeepalive = 20Example configuration file in addition to peerAfter bringing up the interface, we can see not only the interface configuration but a peer configuration!Full configuration for the endpointOnce more, we need to log the public key and adjust our Jump config.JumperGo ahead and replicate the peer configuration we created on the endpoint, exchanging the appropriate variables:[Interface]PrivateKey = 4IUXksWM2WyPWr3jAmmbRDxLNdDTDKTiOfIr1pLhaEg=Address = 10.40.0.1/24ListenPort = 12342[Peer]PublicKey = zQerI54YmapcmodE+OpZSumMe0GknZPlVC3KHXT0I2o=AllowedIPs = 10.40.0.2/32Endpoint = 10.10.69.30:12342PersistentKeepalive = 20However, we can‚Äôt ‚Äúup‚Äù and already active interface so we must down it then up it to make the changes take effect.wg-quick down wg0 &amp;&amp; wg-quick up wg0Active Wireguard Tunnel!Just like that, we have a connection between the two hosts that is lightweight and encrypted. We can verify it by pinging one of the IP‚Äôs inside the tunnel:We can reach the endpoint‚Äôs IP ‚ÄîSecuring SSHCreating the tunnels between the Jumper and the endpoint is only part of the configuration. The other part of it is securing SSH on the endpoint. More specifically, putting in the line ListenAddress &lt;ip&gt;This config option will make the OpenSSH server listen on the IP specified and no others. You can have multiple entries for this option for example if you want to add some redundancy to this setup.There are also other options which may prove useful in the config. For example, disabling root SSH login or changing the port. Here is my list of options.Debian: nano /etc/ssh/sshd_configRocky/RHEL: nano /var/lib/systemd/system/sshd.serviceListenAddress 192.168.174.11ListenAddress 192.168.53.5ListenAddress 10.40.0.2Port 7184PermitRootLogin noPubkeyAuthentication yesAuthorizedKeysFile\t.ssh/authorized_keys .ssh/authorized_keys2PasswordAuthentication noPermitEmptyPasswords noChallengeResponseAuthentication noUsePAM noX11Forwarding yesAcceptEnv LANG LC_*There are also other options you can do such as disabling TTY outright and forcing /sbin/nologin however I do not force this at the moment.How to connectConnecting to one of your boxes through the Jumper can be accomplished multiple ways. The easiest way is to store the SSH data in ~/.ssh/config. This file follows a specific syntax to outline hosts and other miscellaneous SSH details such as host names and port numbers. Alternatively, you can use arguments in the SSH command itself to dictate where to hop to and from.SSH ConfigThe SSH config file can hold traits for each individual connection or wildcard traits. For example, if all your services use port 7184 for SSH, you could deploy this. I would advise reading the man page to learn more.nano ~/.ssh/configHost Jump User jumper Hostname 192.168.53.1Host Mail User mail Hostname 192.168.53.5 ProxyJump JumpHost Mailbackup User mail Hostname 192.168.53.6 ProxyJump JumpHost NS1 User dns Hostname 192.168.53.7 ProxyJump JumpHost * Port 7184After creating this file, you can connect to each host via typing:ssh &lt;name&gt;For example, connecting to my NS1 would be:ssh NS1SSH ParametersIf you do not want to create a manual configuration file, you can replicate this inline with the SSH command. Similar to the above, if you wanted to SSH to the NS1 server we would do the following:ssh -J jumper@192.168.53.1 -p 7184 dns@192.168.53.7 -p 7184The syntax of it of course being:ssh -J &lt;user&gt;@&lt;jumper_ip&gt; &lt;user&gt;@&lt;endpoint_vpn_ip&gt;ConfirmationAfter configuring this all, we can verify that not only is it going via the SSH tunnel but also that SSH is not listening on any public interfaces.Going through the VPNIf we check /var/log/auth.log (or /var/log/secure on RHEL) we can see that SSH sessions have been established via our tunnel address, more specifically the jumper address:Aug 17 21:07:16 hostname sshd[6296]: Accepted publickey for root from 192.168.53.1 port 1234 ssh2: RSA SHA256:ABCdefg...Aug 17 15:07:46 hostname sshd[1343532]: Accepted publickey for root from 192.168.53.1 port 1234 ssh2: RSA SHA256:ABCdefg...Another quick way to verify this is by using the w command: 21:11:12 up 6 days, 20:46, 3 users, load average: 0.00, 0.00, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATdns pts/0 192.168.53.1 21:07 0.00s 0.00s 0.00s wVerifying SSH only listens on the VPN addressThere are multiple ways to see what net connections are active on a Linux host. The one I always remember is netstat -tulpn. Paired with a grep command, we can filter it to specifically show only the SSH service:root@hostname:~# netstat -tulpn | grep 7184 tcp 0 0 10.40.0.2:7184 0.0.0.0:* LISTEN 2448/sshdAs we can see in the above snippet, it is only binding to the address 10.40.0.2 as opposed to 0.0.0.0." }, { "title": "pfSense Routing with SR-IOV and Proxmox", "url": "/posts/pfsense-sriov/", "categories": "", "tags": "Linux,, Networking,, Virtualization", "date": "2021-07-04 00:00:00 +0100", "snippet": "IntroductionSingle Root I/O Virtualization (SR-IOV) is a technology that was developed in order to split up physical PCI devices into multiple separate PCI devices. In a virtualization stack, it allows us to remove the VMM layer of virtualization and address hardware directly. Scott‚Äôs Weblog explains this in further detail. Source: https://doc.dpdk.org/guides/nics/intel_vf.htmlOne of the most common examples of this technology is present in NIC‚Äôs. Intel has put this technology in lots of there equipment, but one of the most common chip-sets for Homelabbers is the i350 chipset. After doing my research online, I came to the conclusion that this was the most appropriate chip-set for pfSense. My specific variant of this chip-set is the T4V2 version. A 4 port gigabit NIC going for about ¬£50 on eBay.SR-IOV allows me to split up my WAN connection into ‚ÄúVirtual Functions‚Äù which reduces the overhead on the Hypervisor due to not having to process traffic through the virtual switch. This means that latency-sensitive applications could benefit greatly alongside more CPU cycles being available to the host.Creating Virtual FunctionsI didn‚Äôt find much documentation on this available. However, after a couple of bug reports and Proxmox threads I had managed to piece it together. Here are the requirements: SR-IOV enabled in the BIOS. I found my NIC has a dedicated section with the amount of VF‚Äôs I could define. Intel VT-D / AMD-Vi enabled Access to GRUB‚Äôs configuration file Access to module configurationsEditing GrubOn Proxmox, the path for the grub configuration file can be found at /etc/default/grub. Under the GRUB_CMDLINE_LINUX_DEFAULT parameters I appended the following:amd_iommu=on iommu=pt pci=assign-busses (Replace AMD with Intel for Intel CPU‚Äôs)These flags will split the PCI devices up into IOMMU groups and assign numbers from the kernel as opposed to the firmware. I found that Virtual Functions could not be created without the assign-busses flag. This thread is how I found out.Warning: pci=assign-busses will rename any interfaces you have, please ensure you have console access to amend your network conneciton.My Grub configuration with the changes highlighted in red. After adding this to my grub config file, I ran update-grub. My previous PCI pass through devices had changed identifiers so I made sure to update these also,Enabling the VFIO modulesWe need to add the VFIO modules which will allow us to use the VF‚Äôs as a stub so that they do not act as network interfaces on the host. This is a common technique to keep a PCI device free from the host in order to pass through to a guest machine.Put vfio, vfio_iommu_type1, vfio_pci and vfio_virqfd into /etc/modulesA preview of my modulesAfter this, we can update our initramfs image with update-initramfs -u -k all and reboot the host with shutdown -r now.Enabling the functionsCredit to Sandbo on the Proxmox Forum for a detailed guide on this. These gave me the right pointers in order to enable it for my system.To begin with, we will need to get the device name of our NIC. This can be found with the command ls /sys/class/net as it will list all available interfaces. Hopefully this is enough for you, but the command ip address or ip link may also help to indicate which NIC is what. For me, my i350 was under the name enp6s0f0.My interface with the ip link commandNow that we have our NIC interface, we can finally execute the following. Make sure to replace N with the amount of interfaces you want and with the interface name we found above.echo N &gt; /sys/class/net/&lt;IF&gt;/device/sriov_numvfsIn my case, the command is:echo 6 &gt; /sys/class/net/enp6s0f0/device/sriov_numvfsThis command will hopefully succeed, creating as many virtual interfaces as you‚Äôd like. A great way to see these interfaces is by using the ip link comamnd:The output of ip link showing my VF‚Äôs.‚ÄîMaking the functions persistentSandbo made a perfect systemd daemon for this. Credits to him.Create the file /etc/systemd/system/sriov.service and input the following, amending to follow the previously used command:[Unit]Description=Script to enable SR-IOV on boot[Service]Type=oneshotExecStart=/usr/bin/bash -c ‚Äò/usr/bin/echo N &gt; /sys/class/net//device/sriov_numvfs‚Äô[Install]WantedBy=multi-user.targetIf you intend to use pfSense with a virtual function, you will need to manually set the MAC address of one due to broken support in FreeBSD:ExecStart=/usr/bin/bash -c '/usr/bin/ip link set &lt;IF&gt; vf &lt;NUM&gt; mac 02:00:00:00:00:00'This script will run while the server is starting up, alleviating the need for manual VF creation.Passing to a Virtual MachineNow that our Virtual Functions have been stubbed and are working correctly, we can pass them through to our guest machines. These machines will more than likely have the driver already, however BSD guests may face issues. I found that OpenBSD did not support VF‚Äôs.On the web interface for Proxmox, select a virtual machine on the sidebar and click on the hardware section. Next, click:Add &gt; PCI Device &gt; Virtual Function XA list of PCI devices we can pass through. Select a Virtual FunctionYou may notice on the side each VF has a different IOMMU group. ¬†This can be a easy way to note down which guest uses what, or alternatively the PCI ID of the VF.That‚Äôs all. It‚Äôs a relatively simple process, however getting here may have been a few headaches. In the future, all you need to do is just assign PCI devices!Monitoring the interfaceOn the guestOnce you start the guest, it will more then likely have already configured itself and be online within your network. In my case, it has received an IP from DHCP:The VF is represented by ens16If we view information about the PCI device (your ID will be different):Information on the ethernet via passthroughWe can see that it is detected and has a driver in use.On the hostIf we view out interfaces with the ip link command, we can see that there has been a change in the VF MAC addresses:VF 3 has a random MAC addressThis shows us that the interface has been successfully configured for pass through as the driver as given it a valid MAC. However, this may not always be the case on UNIX derived guests and may need manually setting. This can be done with the following command:ip link set &lt;interface&gt; vf &lt;number&gt; mac 02:00:00:00:00:00We can also view the interface with commands such as nload or iftop by finding the interface name associated with the VF. In my case, this VF carried the name fwln101i0The interface on nload‚ÄîpfSense quirkspfSense can work lovely with Virtual Functions, however, as mentioned above BSD guests do not tend to treat these interfaces well. In my experience, they either do not assign MAC addresses automatically or they do not have the drivers at all.To fix this, I amended the following to my systemd script:ExecStart=/usr/bin/bash -c '/usr/bin/ip link set enp6s0f0 vf 0 mac 02:00:00:00:00:00'This will assign the MAC address 02:00:00:00:00 to the first function on system boot. This function will then be passed through to my pfSense guest.After this, the guest should recognise the function and work correctly. I have not met any unexpected issues by using this MAC address fix however I cannot guarantee the same for you.Shows up under the igb0 interface‚ÄîFinal ThoughtsAs a consumer, this technology is not something which I can benefit from as much as someone from the enterprise may. None the less, exploring this route of Virtualization is opening doors for further projects. Youtuber Level1Techs has explained in great detail about how SR-IOV can open up single GPU virtualization for multiple guests which is exponentially more exciting.If you have a chipset laying about which supports this technology, it‚Äôs worth a play around to grasp the potential of it." }, { "title": "JTAG Hacking a Xbox 360 in 2021", "url": "/posts/jtag-install/", "categories": "", "tags": "xbox,, free60", "date": "2021-02-26 00:00:00 +0000", "snippet": "The Xbox 360 is a pretty important console to me. I only started playing around late 2011, but it still provided countless hours of entertainment and the idea of a modding the console really appealed to me. Since then, I have learned a lot from the process such as basic system security and electronic skills.The original KK exploit was a stepping stone to the SMC (JTAG) hack. The SMC hack will keep the JTAG ports enabled on the motherboard and by bypassing the eFuse technology, it allowed for a downgrade back to the 4532 kernel and rebooting into a custom firmware. I would recommend ModernVintageGamers video on this if you would like to learn more: [How the Xbox 360 Hypervisor Security was Defeated MVG](https://www.youtube.com/channel/UCjFaPUcJU1vwk193mnW_w1w) Identifying the vulnerability and your consoleThe JTAG exploit is only vulnerable on PHAT consoles which have not been updated past summer 2009. This correlates to Kernel 7371. However, there are some slight exceptions. Notably, special edition Jasper consoles (2009) are not vulnerable such as the Resident Evil 5 and MW2. My friend purchased a new RE5 console for multiple hundreds to find out it‚Äôs not vulnerable, don‚Äôt do the same.For reference, this is how to find out your motherboard type:Motherboard identification guideOut of these consoles, you ideally would have a Jasper motherboard. These output the least amount of heat and have good heat sinks to prevent the infamous RRoD. However if you take a look at the date, there was a very small window in which these were vulnerable. Also, v2 Jaspers can be had before September. For example, the Arcade 512MB Big Block.When it comes to dumping your NAND, you can also identify if your console is vulnerable by the CB value. If it is on the following list, you are out of luck: Xenon: 1922, 1923, 1940, 7373Zephyr: 4571, 4572, 4578, 4579, 4580Falcon/Opus: 5771Jasper: 6750 I have had 3 Big Block JTAG Jaspers, and they have all been on CB 6723 which is vulnerable. I cannot comment for any other motherboard revision.So in short, you are looking for: Dashboard 7371 or below (check CB for 7371) Ideally a HDMI console made on 2008-2009Doing the hack - ComponentsThere‚Äôs multiple wiring routes of doing the hack but I did the original wiring method. However this can differ by motherboard. In the case of a Xenon, the layout is slightly different to the rest of the 360 line. But among both methods, these are the components required: 2x 1N4148 Diodes 26-30 AWG Kynar wiring A NAND-X/JR-Programmer or a Matrix SPI Nand Flasher Soldering Iron Solder (60/40 rosin core recommended) FluxIdeally you would also have: Heatshrink tubes Electrical tapeFor the hack you will also need the following software: JRunner NAND device driversDoing the hack - NAND wiringTo proceed with the hack, we need to read the NAND of the console and ensure that we have at least 2 matching dumps. We will use these to create freeboot (hacked) images of our NAND that will allow for unsigned code to run. They can also be used to later restore to in the event that you might want to revert the hack.NAND-X / JR-Programmer wiringMatrix SPI Nand Flasher wiringBelow are some *rough *images of how I did my install. Take note that I decided to use the hard drive connector as ground. This is much easier than using the point on the motherboard and I would personally recommend it to avoid unnecessary complications. Also ensure that you ‚Äòtin‚Äô the points (pre-apply solder). This creates a much easier surface to attach wires to.Using HDD prong for groundTinned points on the motherboard for NAND reading. This is recommended.JR-Programmer installedIt‚Äôs not too important that these wire‚Äôs aren‚Äôt very neat. They are only temporary while we do operations with the NAND.Doing the hack - NAND Reading and WritingAt this stage, you should have the NAND headers installed. We will need JRunner to continue from this point. It can be found as a mirror on my website from here. Note that this install is covering the dashboard 17599. I expect this to be the final dashboard for the console, but if it‚Äôs not, it‚Äôs preferred you use the latest dashboard (although not required).Open up JRunner. You should be presented with a screen like this:JRunner dashboardIt‚Äôs important that your NAND device is detected. Ensure the drivers are installed and hopefully you will see an image of it in the center under the write Nand button.JRunner dashboard with JR-Programmer detectedNext, click the ? next to the Motherboard box. Make sure your console has power BUT IS NOT POWERED ON.If you power on your console while attached, you** WILL break** your programmer.This will query the flash config of your console, and if your wiring is correct, will return details about your console. Hopefully you have identified it as you have previously via the power+output or date of the console. If it worked, the output should fill with a flash config and a guess at your console type. Don‚Äôt worry too much if it‚Äôs not accurate, it just needs to work.The ? to query the consoleNext, click Read Nand. I would recommend you do it 3 times however 2 is enough. This ensures that it will dump it and the checksums are the same and thus meaning the NAND dumps are the same. If you see a mention of bad blocks, don‚Äôt be alarmed. JRunner will map these to reserved areas which are used exclusively for this. I recommend you read this post on Free60 for more information.JRunner reading my NAND After you have 2 reads, ensure that it says they areMATCHING NANDS. DO NOT PROCEEDIf they are not matching NAND‚Äôs. These are your lifeline to verify any problems with your console.Tick the JTAG box on the side with no other options. We are not using the AUD_CLAMP alternative install for this.After this, click create XeLL Reloaded image and then proceeded by clicking Write XeLL Reloaded. Creating a Xell-Reloaded image and writing.At this point your console should have the XeLL image written to the first 50 blocks. XeLL (Xenon Linux Loader) is used to grab our CPU Key which will decrypt our NAND and allow for us to reboot into a 4532 kernel. Disconnect your JR-Programmer but keep it soldered. We need it later.Doing the hack - Physical InstallNow we get to do the fun stuff. Installing the JTAG hack. Below are some diagrams of how to install it for respective motherboards. Please note, the second image depicts a LPT cable. This is not required and only there for legacy methods.Xenon Motherboard JTAG wiringZephy, Opus, Falcon, Jasper Motherboard JTAG wiringSome people prefer to use the diodes with direct connections to each point. However, I opted to use a bit of wiring with them when installing. This created a cleaner look but is functionally identical. Make sure you orientate the image the correct way when looking at it. However, ensure that you use shrink wrap tubes to prevent any sort of shorting. This is highly recommended!There‚Äôs not much more to be said about this. It‚Äôs a relatively simple proceedure. Some guides will tell you to solder to the RF plate, but this is completely unecessary. Below is my wiring (following this guide).Final wiring. Note: Diodes are covered in shrink wrap but can be seen at the endReplacement RF Board wiring. This was cleaned up and taped down afterAfter this, the hack should be complete. UNPLUG YOUR NAND DEVICE.Put on your RF Plate and power up the console. Make sure to put in a video signal as well as a Ethernet cable. This will make it much easier to get your CPU KeyDoing the hack - Creating and Writing FreebootAfter the hack is installed and we have our CPU key, we want to turn off the console so that we can write our freeboot image. This will look like the regular dashboard but with the ability to run unsigned applications. If you do not want to write out your CPU Key, put the IP your console got in the bottom right and click Get CPU key.On JRunner, ensure your NAND dump is on the source file and click Create XeBuild image. Once created, it will output a log of information such as your hack type, CPU key and other misc. information. These will be saved.JRunner creating a XeBuild imageAt this point, you are at your final step. TURN OFF YOUR CONSOLE.Click Write Nand and it should begin to write the XeBuild image. After this, unplug everything INCLUDING THE NAND READERFrom your console and re-plug power and video and power on. Hopefully, you should be presented with the stock latest dashboard with a missing avatar.FinaleThat‚Äôs it. You might want to set a custom fan curve using a software like Dashlaunch and install XeX menu so you can browse the files on your console. These are paramount on any modded system, especially a phat, to help prevent a RRoD occurring.It‚Äôs quite satisfying to load Dashlaunch and be presented with a JTAG hack. There‚Äôs a lot of significance to it as they are hard to come by. For note, you can always check your flash and console type in Dashlaunch at the bottom right.Dashlaunch showing the JTAG hack type‚ÄîFinal ThoughtsThe JTAG hack was a very significant breakthrough in the ability to run homebrew on your own console. Even today, RGH or JTAG consoles are a great step up from the original XBOX homebrew scene. Emulators are present, lots of documentation on developing your own software and even the ability to play backup copies of your games. Lots of PHAT consoles experience a sticky or non-functional disc tray and this can be a great way to revive a console, assuming you have the hard drive space.I would recommend the 360 to someone who is interested in capabilities of homebrew consoles. but I would not recommend it to someone who wants to mod games online. XBLive is nourished with people on modified consoles and I have my doubts at how long XBLive will even last. I‚Äôd imagine it also peaked a lot of interest from kids on how security and electronics work. I sure learned a lot from my years of using on." }, { "title": "Managing edge nodes with Docker and GitLab", "url": "/posts/deploying-nginx-configuration-files-to-edge-nodes-with-gitlab-cd-docker/", "categories": "", "tags": "Linux,, Web,, Git,, Docker,, Container,, NGINX", "date": "2020-12-30 00:00:00 +0000", "snippet": "This post is a continuation of my previous post, ‚ÄúDeploying a ‚ÄòCDN‚Äô with GeoDNS‚Äù.Reviewing this in 2023 - This is when I knew little about containers. Consider this a ‚Äòtest poc‚Äô ;)One of the many challenges I faced when creating my CDN was a method of updating the vHost configuration files among multiple PoP‚Äôs. At first I was manually creating and modifying the conf files on a base, none-containerised install of NGINX (CentOS 8). It dawned on me when working with a client from CubeOps to install Ghost in a container, that this would be the prefect time to finally get around to learning how to utilise docker for myself.When researching how I should handle this, I realised that I had to learn how to create my own images and how to host them before deploying them. Hosting them at a service like Dockerhub does work, but there is a significantly less learning curve involved in that and giving out my data to GitHub is not a preferred method. Especially when it involves private keys‚Ä¶The solution I settled on initially was hosting the registry and gitlab on the same machine in a separate container. Although I soon realised that GitLab had its own integration and container registry which would prove to make it significantly easier and less ‚Äúhacky‚Äù.Below is some of the steps I took to deploy this service:Base system requirementsA server with the following criteria: At least 4GB of RAM 1 semi-powerful CPU core At least 10GB of storage A Linux distro of your choice (I chose Debian 10) Container technology capabilities.I found that OpenVZ does not integrate well with containers by default unless the host specifically enables it. As such I recommend KVM.DNS records ¬† ¬† ¬† ¬† Name Type Value Required gitlab A IP Yes registry CNAME gitlab No The registry sub domain is not required unless you do not want to connect to your registry via a remote port. We will expose the registry via HTTPS later on. Feel free to setup a reverse proxy for it.Installing GitLab (via Docker)Installing GitLab is not a challenging task thanks to the power of containers. In fact, the hardest part about it would actually be the decision of how you would like to store your data. For this I decided that it would be best to store my configuration files on the host under /srv/gitlab.docker run ‚Äìdetach‚Äìhostname gitlab.example.com‚Äìpublish 443:443 ‚Äìpublish 80:80 ‚Äìpublish 5050:5050 ‚Äìpublish 5050:5050‚Äìname gitlab‚Äìrestart always‚Äìvolume /srv/gitlab/config:/etc/gitlab‚Äìvolume /srv/gitlab/logs:/var/log/gitlab‚Äìvolume /srv/gitlab/data:/var/opt/gitlab‚Äìenv GITLAB_OMNIBUS_CONFIG=‚Äùexternal_url ‚Äòhttps://gitlab.example.com/‚Äô;‚Äùgitlab/gitlab-ce:latestThe above command will automatically pull and run a gitlab community edition image**. **If you want to deploy the enterprise edition, change it to gitlab-ee:latest. It will then allow the container to bind to the ports 80, 443, 5050 &amp; 5555 which is all required. Next, it sets the omnibus variable within GitLab for our URL to https://gitlab.example.com. This variable will automatically deploy a LetsEncrypt certificate if there is HTTPS present at the start which we will utilise later on. Finally, we will name the container ‚Äúgitlab‚Äù and make sure it automatically starts when the Docker daemon is loaded.You can check the logs of this container by doing:docker logs gitlabGitLab should now be live at https://gitlab.example.com and accessible over HTTPS. After setting up a root password, you can login and be presented with your personal projects.We will be utilising the admin page which can be accessed by the wrench on the top taskbar. This is so that we can setup a GitLab runner which will be used for building the dockerfiles‚ÄîCreating a GitLab runner (via Docker)GitLab requires runners in order to compile and test your commits. In our case, they will test our dockerfile, NGINX vHost configurations and deploy to the edge nodes.Setting this up proved to be difficult at first but after some researching I found that in order to make this work with docker we essentially have to deploy a ‚Äúdocker in docker‚Äù image which will pass through the hosts docker socket. In addition, we will need to make it a privileged runner otherwise it will not have the right permissions to run these tests. To begin with, head to the admin panel and then click runners.Runner tab on the leftAfter this, you should be presented with options about adding a new runner. In this case, we want to copy the token that it provides. We should also note down the URL, which should be the same as your gitlab instance. GitLab runner manual tokenOnce we have this, head over to your Linux box and put in the following:docker run --rm -it -v /srv/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner register -n \\--url $GITLAB_URL \\--registration-token $REGISTRATION_TOKEN \\--executor docker \\--description \"Docker Runner\" \\--docker-image \"docker:stable\" \\--docker-privilegedThis should then register the runner with your GitLab instance. However, we passed through the RM command which means it will exit after the command ends. To run the runner permanently, we need to once more run it.docker run -d --name gitlab-runner --restart always -v /srv/gitlab-runner/config:/etc/gitlab-runner -v /var/run/docker.sock:/var/run/docker.sock gitlab/gitlab-runner:latestSetting up a Container Registry (Via GitLab)The container registry on GitLab is quite simple to setup. Earlier when we deployed GitLab we passed through the port 5050 &amp; 5555. This will be used for the container registry and a HTTP mirror of it.To begin with, we will want to edit the file **/srv/gitlab/config/gitlab.rb **on the hostand find ¬†the following lines:registry_external_urlregistry_nginx['ssl_certificate']registry_nginx['ssl_certificate_key']registry_nginx['enable']registry_nginx['listen_port']If you are unfamiliar with basic file editing principles, do the following. It will take you to the sections of the config where these are stored: nano /srv/gitlab/config/gitlab.rb CTRL + W registry_external_url CTRL + Wregistry_nginx['enable']We then want to edit these values to the following, replacing gitlab.example.com with your instance url.registry_external_url = ‚Äòhttps://gitlab.example.com:5555‚Äôregistry_nginx['ssl_certificate'] = \"/etc/letsencrypt/live/gitlab.example.com/fullchain.pemregistry_nginx['ssl_certificate_key'] = \"/etc/letsencrypt/live/gitlab.example.com/privkey.pemregistry_nginx['enable'] = trueregistry_nginx['listen_port'] = 5050docker exec gitlab gitlab-ctl reconfigureCreating the repository &amp; GitLab CI instructionsWhen deploying the app to my edge nodes it first needs to be built and tested. To begin with, I added my initial NGINX configuration files then created a Dockerfile and .gitlab_ci.yml file.All the files in my repository. Note: This is after my pushes.The way my ‚ÄòCDN‚Äô works is it caches a web request from my origin server and serves it. The Dockerfile will grab a nginx:mainline-alpine image and copy the vHost files to the respective directories (~/conf.d/) and then copy the SSL configuration files to ~/live/. Finally, it will test the vHost files and then start NGINX.FROM nginx:mainline-alpineRUN rm /etc/nginx/conf.d/default.confCOPY jamdoog.conf /etc/nginx/conf.d/jamdoog.confCOPY blog.jamdoog.conf /etc/nginx/conf.d/blog.jamdoog.confCOPY analytics.jamdoog.conf /etc/nginx/conf.d/analytics.jamdoog.confCOPY fullchain.cer /etc/nginx/live/fullchain.cerCOPY jamdoog.com.key /etc/nginx/live/jamdoog.com.keyCOPY options-ssl-nginx.conf /etc/nginx/live/options-ssl-nginx.confCOPY ssl-dhparams.pem /etc/nginx/live/ssl-dhparams.pemCOPY nginx.conf /etc/nginx/nginx.confRUN nginx -tEXPOSE 443CMD [\"nginx\", \"-g\", \"daemon off;\"]The gitlab CI file will do the following:Pull the Docker-in-Docker imageDefine the following stages: Build Release Deploy TestGrab the dockerfiles and build the imageDeploy the image to the container registryRun a script on each edge node to pull the latest dockerfile and deploy itCurl each edge node to verify the page is workingI cannot share the file because it would compromise my security but there is alot of documentation available online on how to use Gitlab CI.EvaluationMy workflow with GitLab CD and my own container registry has created the ability to deploy changes to my edge nodes within a minute. Instead of spending 10 minutes modifying NGINX configuration files globally, all I have to do is commit a push to my GitLab server.However, there is some minor problems with this approach. Docker is meant to be used for scalability and the way I have set this up does not attempt it properly. My original plan was to use Docker Swarm which while actually would have worked as intended, I wanted it to show the edge node at https://jamdoog.com as opposed to the master‚Äôs node. I decided I would then operate the edge nodes as independant docker masters.Another problem with this approach is that when I deploy to my edge nodes, it is not as clean of an approach as I would like. However, in a production system I would adapt this to a method which should actually be used in production.What I would changeWhen doing this whole project over again, I would absolutely reconsider the whole topology of the ‚ÄòCDN‚Äô and how data is served. As mentioned, caching the web server works but traditional CDN‚Äôs will store assets on fast disks or even RAM. It‚Äôs quite clear that this is a makeshift solution on a budget. When services such as BunnyCDN and Cloudflare exist, it‚Äôs impossible to recommend this unless it‚Äôs strictly for learning.However, the GitLab workflow outlined in this post is great and it is not something I would change. Obviously it is not the intended use, but it has been a greater learning curve than a couple of rsync commands in loops." }, { "title": "Deploying a \"CDN\" with GeoDNS", "url": "/posts/deploying-a-cdn-with-geodns/", "categories": "", "tags": "Linux,, Web", "date": "2020-12-16 00:00:00 +0000", "snippet": "IntroductionWhen wondering how to pass the time I decided to optimise my website. I took initial steps such as minimising CSS and JS however incase you haven‚Äôt seen, my (old) base site actually is actually very small (and in reality serves no purpose).At the time my website was powered by 2 loadbalancers and 3 web servers located around the world. If you was curious, the setup was HAProxy following this tutorial. Also, at this time, I used OpenBSD and had no idea how to use PF so my HTTP connections were left exposed at the origin‚Ä¶This worked however, it wasn‚Äôt as satisfying as saying ‚ÄúI deliver content from a local edge node‚Äù. Not only that, but with big companies like Cloudflare powering my websites since 2016-2019 the idea of creating my own ‚ÄúCDN‚Äù really appealed to me from a privacy standpoint but also a learning standpoint.As it stood my web payload was already tiny and in practicallity the only issue for loading times was DNS requests and web server location. Infact, Google PageSpeed insights suggested it was practically perfect (100%). But I knew it wasn‚Äôt optimal for people viewing from Asia or America.Servers?So the next step logically to me was to get some new PoP‚Äôs. I am very thankful that LET is well known for there Black Friday deals! The aftermath of that I bought 4 VPS servers, 3 in America and 1 in Singapore. Shoutout to Racknerd and Cam for some awesome deals. At the time of writing this 2 of the America servers are not used (Infact 1 has not been delivered, thanks Virmach).What servers do I currently use?OVH: London, United Kingdom Gravelines, France Beauharnois, Canada Frankfurt, Germany Singapore, SingaporeHetzner: Falkenstein, Germany Helsinki, FinlandColocrossing: Ashburn, Virginia, United StatesServerius: Droten, The Netherlands (coming soon)ResearchThe next step was to figure out how I could utilise these. I actually considered GeoDNS from the very start however I needed to further my research before deciding it would be the route I took.My criteria for a self-hosted CDN was the following: ¬† ¬† Cannot rely on external providers such as AWS (No thanks Route53) ¬† ¬† Had to be configured by myself from origin server to end server ¬† ¬† Had to be scalable. More on this later ¬† ¬† Some sort of learning involved Posts such as this one from NGINX really helped to inspire me, I highly suggest you watch the presentation. Of course they are at a much larger scale, but the principle was the same; serve content faster. Although I quickly found that Google was not that useful as it proposed alot of advertisements and/or solutions outside of my reach. How I learned to Stop Worrying and Build My Own CDN | Time WarnerThe big one that, to my understanding, is used in Industry is BGP anycast. Of course, this is completely out of reach and not possible for a University Student who doesn‚Äôt have thousands of pounds available. Also, none of my providers allow BGP announcements either.After going through multiple pages of search queries I eventually settled on the GeoDNS solution. I came accross a gold mine of a page, GeoIP.site. Who would have thought someone wanted to achieve the same goal?How does it work?The author of this page explains that they wanted a in-house soltion to move away from UltraDNS and that this was the solution they utilised. While I encourage you to read the original post, to abbreviate it, the scripts provided will pull from Maxmind and other GeoLocation databases and format them to be ACL compatible with BIND.With these IP ranges matched to countries, it allows for custom DNS responses based on the origin IP. Honestly, such a simple solution amazed me. Of course this isn‚Äôt a foolproof method and there are some flaws in it but, this will be talked about later.GeoDNS diagram outlining how the connection worksHow do my edge servers provide content?NGINX. Reverse proxying is a beautiful technology which allows me to cache my website locally on each edge node once they have fetched before. I did look into varnish however after researching the differences between this approach I felt like NGINX was the most appropriate.Speed Statistics?I unfortauntely didn‚Äôt note down any previous measurements for speed. However, I can provide some figures of how this preforms as of now.Google PageSpeedGoogle PageSpeed - DesktopGoogle PageSpeed - MobilePingdom - LondonPingdom - JapanPingdom - United States (Washington)WebPageTest - United States (Salt Lake City)Uptrends - NetherlandsGTmetrix - CanadaSite 24x7 - Hong KongDownsides to this approachDNS isn‚Äôt a perfect technology and I have noticed that some lookups do have exceedingly high times (6+ seconds). While this is not true accross the board (as seen in above screneshots) it has happened at least a couple times. I believe this is down to my configuration of named and it‚Äôs ACL pool.Of course depending on the DNS resolver that the client is using it can seriously affect the results of this. For example, if someone uses Google or Cloudflare it might has potential to return results based on these resolvers last saved query. However, since the majority of people will utilise ISP nameservers this should not be considered too much.Another critique of this system is that it does not take advantage of city based responses. For example, a US-EAST client connecting to my US-WEST server would have significantly higher loading times compared to if I had a server in US-EASTFinally this approach is not great resolution when downtime occurs. I could create a custom script which updates my nameservers to push clients to another sevrer, however, this does not solve the issue of DNS caching. This is where a solution such as a floating IP address could come into use but I do not have any services capable of this.As a personal issue with this approach, I do not have much content to actually put this system through work. This blog may act as one but it is hardly something to break a sweat over. This is where I expect load balancers in combination with this to be required in ensuring uptime.SummaryIn short a Geo aware DNS resolver can absolutely help to resolve page speed times for content, especially if you have lots of PoP‚Äôs. However, it does also have flaws. I would recommend it in combination with other technologies such as Load Balancing and Anycast.As a fun project though? Learning basic technologies? Sure. It kept me entertained for a couple of days. However, with technologies like Kubernetes around I think this could have been optimised better on my edge nodes." } ]
